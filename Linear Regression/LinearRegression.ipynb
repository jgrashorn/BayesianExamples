{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was written for the course \"Stochastic Finite Elements\" of the Institute for Mechanics and Computational Mechanics at the Leibniz University Hannover in the summer term 2023. Its intention is to show the basic principle of Bayesian updating applied on a linear regression model. I do not guarantee that the implementation or the results are correct. Usage and distribution is permitted, use at your own discretion and risk.\n",
    "\n",
    "Jan Grashorn, June 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to do a linear regression, the model has two (unknown) parameters ($a$ and $b$) and consist of the function $y = ax + b$\n",
    "\n",
    "To keep the notation, we define our random variable as $\\theta = [a,b]^T$ such that the model is a function of both $x$ and $\\theta$:\n",
    "\n",
    "$M(\\theta) = f(x,\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x,theta):\n",
    "    return theta[0]*x + theta[1] # theta[0] = a, theta[1] = b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set initial parameters $a_0 = 2$ and $b_0 = 4$ and generate some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = 2; b0 = 4; # nominal parameters\n",
    "N = 5 # number of points\n",
    "\n",
    "x = np.linspace(0,10,N)\n",
    "y = model(x,[a0,b0])\n",
    "\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that was easy! Now lets add some noise to the model to make it more interesting. We choose here additive noise $c \\sim N(0,\\sigma)$, such that the actual \"measurement\" becomes\n",
    "\n",
    "$y_{meas}(x) = y(x) + c$\n",
    "\n",
    "$y_{meas}(x) = ax + b + c$\n",
    "\n",
    "We can then also plot the results for the original values $a_0$ and $b_0$. We choose here a noise level of $\\sigma = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 3\n",
    "\n",
    "yMeas = y + np.random.randn(N)*sigma\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(x,yMeas,label='data',s=200)\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the objective of this excercise is to calculate (or infer) the actual values of $a$ and $b$ from the measurements of our model output $y$. To do this, we can use for example linear regression.$\\\\$\n",
    "In order to do that, we need to define some sort of error between our model output based on our function input (the function values for a specific value of $\\theta$) and the measured values of $y$. We choose here the MSE (mean-squared error), which is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "MSE(\\theta) = \\frac{1}{N}\\ (M(\\theta) - y_{meas})^2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(theta,x,data):\n",
    "    return np.mean((model(x,theta) - data)**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small distance means a good fit, a large distance is a bad fit. So in order to find a good fit, we need to minimize the distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "sol = opt.minimize(mse,[0,0],args=(x,yMeas))\n",
    "\n",
    "print(\"a_Opt = \",sol.x[0])\n",
    "print(\"b_Opt = \",sol.x[1])\n",
    "\n",
    "yOpt = model(x,[sol.x[0],sol.x[1]])\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(x,yMeas,label='data')\n",
    "plt.plot(x,y,'r-',label='original',lw=5)\n",
    "plt.plot(x,yOpt,'g--',label='linear Regression',lw=5)\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, onto Bayesian stuff! For that we need two things: Prior distribution for our parameters $\\theta$ and the likelihood $p(y_{meas} | \\theta)$\n",
    "\n",
    "For the prior we can assume a uniform distribution, since we have no prior knowledge about the problem. Since a uniform distribution is constant, it does not affect the shape of the posterior distribution, so we can ignore it for now. For the likelihood we can choose a (multivariate) Gaussian distribution with a mean of $y$. This means the likelihood has a dimension of the amount of data points we have measured. We further assume independence in the measurements so that the $N$-dimensional likelihood becomes the product of $N$ one-dimensional Gaussian distributions with mean $y_{meas,i}$:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(y_{meas} | \\theta) = \\prod_i^N \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\text{exp}[- \\frac{(y_{meas,i} - M(\\theta,x_i))^2}{2\\sigma^2}]\n",
    "\\end{equation*}\n",
    "\n",
    "$\\sigma$ is the standard deviation of the measurement error. It can be used as an additional updating variable, but for simplicity we assume it is known here.\n",
    "\n",
    "Usually, the values in the likelihood get very small very fast (because of the multiplication of small numbers in the above example), so often the logarithm is used, which turns the whole thing into a summation:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log p(y_{meas} | \\theta) = N \\log [\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}] - \\sum_i^N \\frac{(y_{meas,i} - M(\\theta,x_i))^2}{2\\sigma^2}\n",
    "\\end{equation*}\n",
    "\n",
    "In this case the likelihood is directly related to the posterior. Recall the Bayesian theorem:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\theta | y_{meas}) = \\frac{p(y_{meas} | \\theta) p(\\theta)}{p(y_{meas})}\n",
    "\\end{equation*}\n",
    "\n",
    "Since the prior distribution and the evidence (the denominator) are constants, we also know the posterior up to a constant. Important: This only works under the assumption that we don't evaluate the function outside of the support of the prior! So in the end, the posterior becomes:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\theta | y_{meas}) = \\frac{p(y_{meas} | \\theta)}{C}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logLikelihood(theta,x,data,sigma):\n",
    "\n",
    "    modelOutput = model(x,theta)\n",
    "\n",
    "    return data.size * np.log(1/(np.sqrt(2*np.pi*sigma**2))) - np.sum((data - modelOutput)**2/2/sigma**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also plot the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlist = np.linspace(0, 4, 100)\n",
    "ylist = np.linspace(-5, 15, 100)\n",
    "X, Y = np.meshgrid(xlist, ylist)\n",
    "Z = np.zeros([100,100])\n",
    "\n",
    "for i in range(100):\n",
    "    for j in range(100):\n",
    "        Z[j,i] = np.exp(logLikelihood([xlist[i],ylist[j]],x,yMeas,sigma))\n",
    "        \n",
    "fig=plt.figure(figsize=(10,10))\n",
    "ax = fig.subplots(1,1)\n",
    "cp = ax.contourf(X, Y, Z)\n",
    "plt.xlabel(\"a\",fontsize=30)\n",
    "plt.ylabel(\"b\",fontsize=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, in principle, we are done, since we have a probability model for our parameter $\\theta$ (save for some constants). But with more complicated models, we arrive at one of the main issues in model updating. Lets look at the (log-)likelihood function again:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log p(y_{meas} | \\theta) = N \\log [\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}] - \\sum_i^N \\frac{(y_{meas,i} - M(\\theta,x_i))^2}{2\\sigma^2}\n",
    "\\end{equation*}\n",
    "\n",
    "The likelihood is directly dependent on the model output. In the case of an analytical function is is not so much an issue since we can just put the expression in there and do some numerical integration (although this also becomes quite complicated very fast) to calculate quantities of interest, such as mean, variance and so on. Calculation of probabilities (especially the denominator in the Bayes' theorem) involves a lot of integration over the whole domain of the likelihood. If you now consider that $M(\\theta,x)$ can also be more complicated model you can imagine that this is in most cases not possible at all.\n",
    "\n",
    "This is why sampling methods are used to explore the PDF with Markov Chains. The basic idea is to start with some state $\\theta_0$. From there, samples are drawn by use of Markov Chains, where each state of the chain depends only on its previous state. A very easy and basic algorithm is the Metropolis-Hastings algorithm. It works by introducing a proposal distribution $p_{prop}$ (different from the one that we are trying to evaluate), from which sampling is easy. For that we use here a Gaussian distribution with the identity matrix as covariance. Depending on the implementation, the proposal distribution is centered at the current state. A proposal sample is then drawn, the values of the PDF in question (the likelihood in this case) are compared and the proposed sample is only accepted with a probability proportional to the ratio between the function values of the current and the proposal sample. Theoretically, this leads to a chain of samples that are distributed according to the target distribution $p_{tar}$, which is exactly what we want.\n",
    "\n",
    "The full algorithm:\n",
    "\n",
    "- Set an initial sample $\\theta_0$\n",
    "- Set $i = 1$\n",
    "- Sample a proposal sample $\\theta_{prop}$ from the proposal distribution $p_{prop}(\\theta | \\theta_{i-1})$\n",
    "- Calculate $\\alpha = p_{tar}(\\theta_{prop})\\ /\\ p_{tar}(\\theta_{i-1})$\n",
    "- Sample $u$ from $U(0,1)$\n",
    "- Set $\\theta_i = \\theta_{prop}$ if $\\alpha > u$\n",
    "- Otherwise set $\\theta_i = \\theta_{i-1}$\n",
    "- Set i += 1 and repeat from step 3 until enough samples are drawn\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it in one dimension first. So we set $a = a_0$ and only look for the value of $b$. We start at $\\theta_0 = 0$. This choice is arbitrary, but the closer we start at the posterior function, the more accurate this becomes. There are of course a lot of other ways to set up these Markov Chains, but the principle remains the same in most (if not all) these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "t0 = 0 # initial sample for the chain\n",
    "C0 = 1 # unit covariance\n",
    "\n",
    "tarFct = lambda theta: np.exp(logLikelihood([a0,theta],x,yMeas,sigma))\n",
    "\n",
    "chain = np.array(t0)\n",
    "\n",
    "tProp = np.random.normal(t0,C0)\n",
    "\n",
    "alpha = tarFct(tProp) / tarFct(t0)\n",
    "\n",
    "print(\"alpha =\",alpha)\n",
    "\n",
    "xlist = np.linspace(-5,10,100) # for plotting\n",
    "\n",
    "tar = np.zeros(100)\n",
    "prop = np.zeros(100)\n",
    "for i in range(100):\n",
    "    tar[i] = tarFct(xlist[i])\n",
    "    prop[i] = st.norm.pdf(xlist[i],t0,C0)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.plot(xlist,tar/np.max(tar),label='Target')\n",
    "plt.plot(xlist,prop/np.max(prop),label='Proposal')\n",
    "plt.plot([t0,t0],[0,1],c='r',label='x0')\n",
    "plt.plot([tProp,tProp],[0,1],c='g',label='Proposal')\n",
    "plt.legend();\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we accept the new sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = np.random.rand()\n",
    "if alpha > U:\n",
    "    tNew = tProp\n",
    "    print(\"Accepted!\")\n",
    "else:\n",
    "    tNew = t0\n",
    "    print(\"Not accepted!\")\n",
    "    \n",
    "chain = np.append(chain,tNew)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. So we can now do this over and over again, keep the accepted samples and throw away the rejected ones.\n",
    "\n",
    "You can run the code below again and again and see what is happening in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xProp = np.random.multivariate_normal(x0,C0,1) # draw one sample from a multivariate normal distribution with mean x0 and covariance C0\n",
    "tProp = np.random.normal(chain[-1],C0)\n",
    "\n",
    "alpha = tarFct(tProp) / tarFct(chain[-1])\n",
    "\n",
    "U = np.random.rand()\n",
    "\n",
    "print(\"alpha = \",alpha)\n",
    "print(\"U =\",U)\n",
    "\n",
    "if alpha > U:\n",
    "    tNew = tProp # accept proposed sample as new sample\n",
    "    print(\"accepted!\")\n",
    "else:\n",
    "    tNew = chain[-1] # reject and use last sample as new sample\n",
    "    print(\"rejected!\")\n",
    "\n",
    "chain = np.append(chain,tNew) # add xnew to chain\n",
    "\n",
    "xlist = np.linspace(-5,10,100) # for plotting\n",
    "\n",
    "tar = np.zeros(100)\n",
    "prop = np.zeros(100)\n",
    "for i in range(100):\n",
    "    tar[i] = tarFct(xlist[i])\n",
    "    prop[i] = st.norm.pdf(xlist[i],chain[-2],C0)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.plot(xlist,tar/np.max(tar),label='Target')\n",
    "plt.plot(xlist,prop/np.max(prop),label='Proposal')\n",
    "plt.plot([chain[-2],chain[-2]],[0,1],c='r',label='x_i')\n",
    "plt.plot([tProp,tProp],[0,1],c='g',label='Proposal')\n",
    "plt.scatter(chain,np.zeros(len(chain)))\n",
    "plt.legend();\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it times more and look at the histogram of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    tProp = np.random.normal(chain[-1],C0/10)\n",
    "\n",
    "    alpha = tarFct(tProp) / tarFct(chain[-1])\n",
    "\n",
    "    U = np.random.rand()\n",
    "\n",
    "    if alpha > U:\n",
    "        tNew = tProp # accept proposed sample as new sample\n",
    "    else:\n",
    "        tNew = chain[-1] # reject and use last sample as new sample\n",
    "\n",
    "    chain = np.append(chain,tNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainMean = np.mean(chain)\n",
    "chainVar = np.var(chain)\n",
    "\n",
    "plt.hist(chain)\n",
    "\n",
    "print(\"Number of samples in chain =\",chain.size)\n",
    "print(\"Mean of chain =\",chainMean)\n",
    "print(\"Variance of chain =\",chainVar)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice. The mean should be somewhere around 4 (as 4 was the value for $b_0$), but we also get some more statistical information about possible values of $b$ dependent on our probability model. Now we can also look at the model output for various values of $b$, such as the average (which would hopefully give more or less the same result as the linear regression), but also upper and lower bounds, as well as things like the $1\\, \\sigma$ and $2\\, \\sigma$ intervals (Of course this is only valid under the assumption that $b$ is normally distributed).\n",
    "\n",
    "Before we do that, we discard some of the samples from the chain, since it needed some time to converge to the true posterior density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chain size before burn-in: \",chain.size)\n",
    "chain = chain[100:chain.size]\n",
    "print(\"Chain size after burn-in: \",chain.size)\n",
    "\n",
    "chainMean = np.mean(chain)\n",
    "chainVar = np.var(chain)\n",
    "\n",
    "plt.hist(chain)\n",
    "\n",
    "print(\"Number of samples in chain =\",chain.size)\n",
    "print(\"Mean of chain =\",chainMean)\n",
    "print(\"Variance of chain =\",chainVar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bMax = np.max(chain)\n",
    "bMin = np.min(chain)\n",
    "bMean = np.mean(chain)\n",
    "\n",
    "bOneSigmaUp = bMean + np.std(chain)\n",
    "bOneSigmaLow = bMean - np.std(chain)\n",
    "bTwoSigmaUp = bMean + 2*np.std(chain)\n",
    "bTwoSigmaLow = bMean - 2*np.std(chain)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(x,yMeas,label='data',s=100)\n",
    "plt.plot(x,y,'r-',label='original',lw=5)\n",
    "plt.plot(x,model(x,[a0,bMean]),'k--',label='b_mean',lw=5)\n",
    "plt.plot(x,model(x,[a0,bMax]),'k--',label='b_Max',lw=2)\n",
    "plt.plot(x,model(x,[a0,bMin]),'k--',lw=2)\n",
    "plt.plot(x,model(x,[a0,bOneSigmaUp]),'b-.',label='b_1sigma',lw=2)\n",
    "plt.plot(x,model(x,[a0,bOneSigmaLow]),'b-.',lw=2)\n",
    "plt.plot(x,model(x,[a0,bTwoSigmaUp]),'g-.',label='b_2sigma',lw=2)\n",
    "plt.plot(x,model(x,[a0,bTwoSigmaLow]),'g-.',lw=2)\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by considering the regression problem in a stochastic setting, we can get more information about the data. The result includes more information than the optimization procedure.\n",
    "\n",
    "We can also of course do it for $a$ and $b$. So let's write the MH-algorithm into a function and update $a$ and $b$ at the same time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mh(func,x0,C0,N):\n",
    "    chain = np.array(x0)\n",
    "    for i in range(1,N):\n",
    "        if x0.shape[1]>1:\n",
    "            xProp = np.random.multivariate_normal(chain[i-1,:],C0,1) # draw one sample from a multivariate normal distribution with mean x0 and covariance C0\n",
    "        else:\n",
    "            xProp = np.random.normal(chain[i-1,:],C0)\n",
    "\n",
    "        alpha = func(xProp.flatten()) / func(chain[i-1,:].flatten())\n",
    "        U = np.random.rand()\n",
    "        if alpha > U:\n",
    "            xNew = np.array(xProp) # accept proposed sample as new sample\n",
    "        else:\n",
    "            xNew = chain[i-1,:] # reject and use last sample as new sample\n",
    "            xNew = np.expand_dims(xNew,0)\n",
    "        chain = np.append(chain,xNew,0)\n",
    "    return chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also do some nice charts with the histograms for $\\theta_1$ and $\\theta_2$ and a scatterplot with the original posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarFct = lambda theta: np.exp(logLikelihood([theta[0],theta[1]],x,yMeas,sigma))\n",
    "\n",
    "t0 = np.array([[0,0]])\n",
    "\n",
    "chain = mh(tarFct,t0,np.eye(2),10000)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(chain[:,0])\n",
    "plt.subplot(2,2,4)\n",
    "plt.hist(chain[:,1])\n",
    "sp = plt.subplot(2,2,3)\n",
    "plt.scatter(chain[:,0],chain[:,1])\n",
    "cp = sp.contour(X, Y, Z) # use plot from before\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discarding the burn-in..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chain[100:chain.shape[0],:]\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(chain[:,0])\n",
    "plt.subplot(2,2,4)\n",
    "plt.hist(chain[:,1])\n",
    "sp = plt.subplot(2,2,3)\n",
    "plt.scatter(chain[:,0],chain[:,1])\n",
    "cp = sp.contour(X, Y, Z) # use plot from before"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modOut = np.zeros([chain.shape[0],N])\n",
    "modOutHigh = np.zeros(N)\n",
    "modOutLow = np.zeros(N)\n",
    "\n",
    "for i in range(chain.shape[0]):\n",
    "    modOut[i,:] = model(x,[chain[i,0],chain[i,1]])\n",
    "\n",
    "for i in range(N):\n",
    "    modOutHigh[i] = np.mean(modOut[:,i]) + 2*np.std(modOut[:,i])\n",
    "    modOutLow[i] = np.mean(modOut[:,i]) - 2*np.std(modOut[:,i])\n",
    "\n",
    "print(\"a_mean =\",np.mean(chain[:,0]))\n",
    "print(\"b_mean =\",np.mean(chain[:,1]))\n",
    "\n",
    "print(\"a_Opt =\",sol.x[0])\n",
    "print(\"b_Opt =\",sol.x[1])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(x,yMeas,label='data',s=100)\n",
    "plt.plot(x,y,'r-',label='original',lw=5)\n",
    "plt.plot(x,yOpt,'b-',label='linear regression',lw=5)\n",
    "plt.plot(x,np.mean(modOut,0),'k--',label='Approx',lw=5)\n",
    "plt.plot(x,modOutHigh,'k-.',label='High',lw=3)\n",
    "plt.plot(x,modOutLow,'k-.',label='Low',lw=3)\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
